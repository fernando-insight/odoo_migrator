{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hermitage migration notebook\n",
    "\n",
    "This notebook app will run the ETL process to migrate Heremitage's Odoo 11 to a new Odoo 15 system.\n",
    "It will basically extract the data from v11 into csv files, make the necessary transformations to make it importable in Odoo 15 and then load the data it into Hermitage's new Odoo 15 instance.\n",
    "\n",
    "## Initial Configurations\n",
    "\n",
    "**Export Configurations File**\n",
    "In this workspace you can find the file [export_connection.conf](export_connection.conf), it is used to set all the necessary settings and credentials to connect to Hermitage's Odoo 11 instance\n",
    "\n",
    "**Import Configurations File**\n",
    "In this workspace you can find the file [import_connection.conf](import_connection.conf), it is used to set all the necessary settings and credentials to connect to Hermitage's Odoo 15 instance\n",
    "\n",
    "**Models Migration Configurations File**\n",
    "In this workspace you can find the file [models_migration_config.conf](models_migration_config.conf). In it its possible to change certain aspects of how the models are extracted and loaded into the new system.\n",
    "Most common possible configurations are:\n",
    "- Fields to migrate\n",
    "- Domain/Filter used to extract\n",
    "- Fields to ignore when loading the data\n",
    "\n",
    "**States Remapping File**\n",
    "In this workspace you can find the file [input_csv_files/States Remapping - Input.csv](input_csv_files/States%20Remapping%20-%20Input.csv.conf), it is used to change the states and countries from certain partners.\n",
    "This file is extracted from a Google Spreadsheet where some states and country data was manually corrected and then mapped with formulas to Odoo's 15 states and country base data.\n",
    "Link to the Spreadsheet to check which states were affected: [States Remapping](https://docs.google.com/spreadsheets/d/1hL9S4APfv9cJYEihGn9Rtimt0xQp7mq1E0ZRpWafa8I/edit?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "- pandas: to make transformations on the data\n",
    "- odoo_csv_tools: Odoo SDK python package specialized for ETL processes\n",
    "- Models' migration config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "from odoo_csv_tools import export_threaded, import_threaded\n",
    "\n",
    "from models_migration_config import models_migration_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "No need to parameterize these values for now\n",
    "Included file paths for some extra csv files that don't follow the standard naming convention of model_name.csv, e.g: res.partner.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPORT_CONNECTION_CONFIG_DIR = 'export_connection.conf'\n",
    "IMPORT_CONNECTION_CONFIG_DIR = 'import_connection.conf'\n",
    "EXPORT_DEFAULT_BATCH_SIZE = 3000\n",
    "IMPORT_DEFAULT_BATCH_SIZE = 1000\n",
    "EXPORT_DEFAULT_REQ_CONTEXT = {}\n",
    "IMPORT_DEFAULT_REQ_CONTEXT = {'tracking_disable' : True}\n",
    "DEFAULT_WORKERS = 2\n",
    "\n",
    "GENERATED_CSV_FILES_PATH = 'generated_csv_files/'\n",
    "INPUT_CSV_FILES_PATH = 'input_csv_files/'\n",
    "STATES_REMAPPING_FILE_NAME = 'States Remapping - Input.csv'\n",
    "PARTNERS_WITHOUT_NAME_FILE_NAME =' res.partner(no name).csv'\n",
    "RES_USERS_GROUPS_FILE_NAME = 'res.users(groups).csv'\n",
    "CRM_TEAM_MEMBERS_FILE_NAME = 'crm.team(members).csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export function wrapper**\n",
    "Created to avoid repeating code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(model_name = None, config = None, domain = None, fields = None, output_file = None, workers = None, batch_size = None, context = None, separator = None):\n",
    "    if model_name in ['ir.model.data', 'ir.model.fields', 'res.groups']:\n",
    "        model_migration_config = {}\n",
    "    else:\n",
    "        model_migration_config = models_migration_config[model_name]\n",
    "    if not config:\n",
    "        config = EXPORT_CONNECTION_CONFIG_DIR\n",
    "    if not domain:\n",
    "        domain = model_migration_config.get('domain', [])\n",
    "    if not fields:\n",
    "        fields = model_migration_config.get('fields', [])\n",
    "    if output_file:\n",
    "        output_file = f'{GENERATED_CSV_FILES_PATH}{output_file}'\n",
    "    else:\n",
    "        output_file = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "    if not workers:\n",
    "        workers = DEFAULT_WORKERS\n",
    "    if not batch_size:\n",
    "        batch_size = model_migration_config.get('batch_size', EXPORT_DEFAULT_BATCH_SIZE)\n",
    "    if not context:\n",
    "        context = model_migration_config.get('context', EXPORT_DEFAULT_REQ_CONTEXT)\n",
    "    if not separator:\n",
    "        separator = model_migration_config.get('separator', ',')\n",
    "\n",
    "    export_threaded.export_data(\n",
    "        config,\n",
    "        model_name,\n",
    "        domain,\n",
    "        fields,\n",
    "        output=output_file,\n",
    "        max_connection=workers,\n",
    "        batch_size=batch_size,\n",
    "        context=context,\n",
    "        separator=separator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import function wrappers**\n",
    "Created to avoid repeating code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(model_name = None, file_csv = None, context = None, separator = None, ignore_fields = None, group_by = None, workers = None, batch_size = None):\n",
    "    model_migration_config = models_migration_config[model_name]\n",
    "    if file_csv:\n",
    "        file_csv = f'{GENERATED_CSV_FILES_PATH}{file_csv}'\n",
    "    elif not file_csv:\n",
    "        file_csv = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "    if not context:\n",
    "        context = model_migration_config.get('context', IMPORT_DEFAULT_REQ_CONTEXT)\n",
    "    if not separator:\n",
    "        separator = model_migration_config.get('separator', ',')\n",
    "    if not ignore_fields:\n",
    "        ignore_fields = model_migration_config.get('ignore_fields', [])\n",
    "    if not group_by:\n",
    "        group_by = model_migration_config.get('group_by', False)\n",
    "    if not workers:\n",
    "        workers = model_migration_config.get('workers', DEFAULT_WORKERS)\n",
    "    if not batch_size:\n",
    "        batch_size = model_migration_config.get('batch_size', IMPORT_DEFAULT_BATCH_SIZE)\n",
    "\n",
    "    if model_name == 'crm.lead.tag':\n",
    "        model_name = 'crm.tag'\n",
    "\n",
    "    import_threaded.import_data(\n",
    "        IMPORT_CONNECTION_CONFIG_DIR,\n",
    "        model_name,\n",
    "        file_csv=file_csv,\n",
    "        context=context,\n",
    "        separator=separator,\n",
    "        ignore=ignore_fields,\n",
    "        split=group_by,\n",
    "        max_connection=workers,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "def import_ignored_fields(model_name, fields = None, file_csv = None,  ignore_fields = [], group_by = [], workers = DEFAULT_WORKERS):\n",
    "    model_migration_config = models_migration_config[model_name]\n",
    "    if not fields:\n",
    "        fields = model_migration_config['fields']\n",
    "    ignore_fields = ignore_fields or model_migration_config.get('ignore_fields', [])\n",
    "    if ignore_fields:\n",
    "        # import the ignored fields\n",
    "        added_fields = list(f for f in fields if f not in ignore_fields and f not in ['id'])\n",
    "        import_data(model_name, file_csv=file_csv, ignore_fields=added_fields, group_by = group_by, workers = workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract res.partner.category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data('res.partner.category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load res.partner.category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data('res.partner.category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract and transform res.partner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'res.partner'\n",
    "model_migration_config = models_migration_config[model_name]\n",
    "\n",
    "export_data(model_name)\n",
    "\n",
    "# Clean-up\n",
    "partners_main_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "partners_dataframe = pandas.read_csv(partners_main_file_path)\n",
    "partners_dataframe.rename(columns={\n",
    "    'categ_id/id': 'category_id/id',\n",
    "    'supplier': 'supplier_rank',\n",
    "    'customer': 'customer_rank'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "partners_dataframe['category_id/id'] = partners_dataframe['category_id/id'].str.replace('False', '')\n",
    "partners_dataframe['type'] = partners_dataframe['type'].str.replace('False', '')\n",
    "partners_dataframe['type'] = partners_dataframe['type'].str.replace('Shipping address', 'Delivery address')\n",
    "\n",
    "partners_dataframe['supplier_rank'] = partners_dataframe['supplier_rank'].astype('str')\n",
    "partners_dataframe['customer_rank'] = partners_dataframe['customer_rank'].astype('str')\n",
    "partners_dataframe['supplier_rank'] = partners_dataframe['supplier_rank'].str.replace('False', '0')\n",
    "partners_dataframe['supplier_rank'] = partners_dataframe['supplier_rank'].str.replace('True', '1')\n",
    "partners_dataframe['customer_rank'] = partners_dataframe['customer_rank'].str.replace('False', '0')\n",
    "partners_dataframe['customer_rank'] = partners_dataframe['customer_rank'].str.replace('True', '1')\n",
    "partners_dataframe['supplier_rank'] = partners_dataframe['supplier_rank'].astype('int')\n",
    "partners_dataframe['customer_rank'] = partners_dataframe['customer_rank'].astype('int')\n",
    "\n",
    "# States remapping\n",
    "states_remapping_file_path = f'{INPUT_CSV_FILES_PATH}{STATES_REMAPPING_FILE_NAME}'\n",
    "states_remapping_dataframe = pandas.read_csv(states_remapping_file_path)\n",
    "\n",
    "for row in states_remapping_dataframe.itertuples():\n",
    "    indexes = partners_dataframe[partners_dataframe['state_id/id'] == row.flash_hlg_state_id].index.tolist()\n",
    "    for i in indexes:\n",
    "        state_id = row.id if type(row.id) == str else ''\n",
    "        partners_dataframe.loc[i:i, 'state_id/id': 'country_id/id'] = state_id, row.country_id\n",
    "\n",
    "partners_dataframe.to_csv(partners_main_file_path, index=False)\n",
    "\n",
    "#Export partner without names\n",
    "fields_to_export = [f for f in model_migration_config['fields'] if f != 'name']\n",
    "export_data(model_name=model_name,\n",
    "            domain=['|', ['name', '=', False], ['name', '=', '']],\n",
    "            fields=fields_to_export,\n",
    "            output_file=PARTNERS_WITHOUT_NAME_FILE_NAME\n",
    "            )\n",
    "partners_without_name_file_path = f'{GENERATED_CSV_FILES_PATH}{PARTNERS_WITHOUT_NAME_FILE_NAME}'\n",
    "partners_dataframe = pandas.read_csv(partners_without_name_file_path)\n",
    "partners_dataframe.insert(len(fields_to_export), 'name','[N/A]')\n",
    "partners_dataframe.rename(columns={'categ_id/id': 'category_id/id'}, inplace=True)\n",
    "partners_dataframe['type'] = partners_dataframe['type'].str.replace('Shipping address', 'Delivery address')\n",
    "partners_dataframe.to_csv(partners_without_name_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load res.partner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'res.partner'\n",
    "#import partners without name\n",
    "import_data(model_name=model_name, file_csv=PARTNERS_WITHOUT_NAME_FILE_NAME)\n",
    "#import the rest of the partners\n",
    "import_data(model_name=model_name, group_by='parent_id/id', workers=1)\n",
    "\n",
    "ignore_fields = ['id', 'name'] + models_migration_config['res.partner']['ignore_fields']\n",
    "import_ignored_fields(model_name, group_by='parent_id/id', workers=1)\n",
    "import_ignored_fields(model_name, file_csv=PARTNERS_WITHOUT_NAME_FILE_NAME, ignore_fields = ignore_fields, workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract and transform res.users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'res.users'\n",
    "export_data(model_name)\n",
    "\n",
    "res_users_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "users_dataframe = pandas.read_csv(res_users_file_path)\n",
    "users_dataframe.rename(columns={'image ': 'image_1920'}, inplace=True)\n",
    "users_dataframe.to_csv(res_users_file_path, index=False)\n",
    "\n",
    "# Export users security groups\n",
    "fields_to_export = ['id', 'groups_id', 'groups_id/id']\n",
    "export_data(model_name=model_name,\n",
    "            fields=fields_to_export,\n",
    "            output_file=RES_USERS_GROUPS_FILE_NAME,\n",
    "            )\n",
    "res_users_groups_file_path = f'{GENERATED_CSV_FILES_PATH}{RES_USERS_GROUPS_FILE_NAME}'\n",
    "users_groups_dataframe = pandas.read_csv(res_users_groups_file_path)\n",
    "users_groups_dataframe.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Export v15 groups\n",
    "export_data(config='import_connection.conf',\n",
    "            model_name='res.groups',\n",
    "            fields=['id'],\n",
    "            output_file='res.groups.csv',\n",
    "            )\n",
    "res_groups_file_path = f'{GENERATED_CSV_FILES_PATH}res.groups.csv'\n",
    "groups_dataframe = pandas.read_csv(res_groups_file_path)\n",
    "groups_dataframe.rename(columns={'id': 'groups_id/id'}, inplace=True)\n",
    "users_groups_dataframe = users_groups_dataframe.merge(groups_dataframe, on='groups_id/id', how='inner')\n",
    "\n",
    "users_groups_dataframe.to_csv(res_users_groups_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load res.users** (and load remaining res.partner fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'res.users'\n",
    "import_data(model_name=model_name)\n",
    "import_data(model_name=model_name, ignore_fields=['groups_id'], file_csv=RES_USERS_GROUPS_FILE_NAME, group_by='id', workers=1, context={'update_many2many': True})\n",
    "\n",
    "#Import remaining res.partner fields\n",
    "ignore_fields = ['id', 'name'] + models_migration_config['res.partner']['ignore_fields']\n",
    "import_ignored_fields('res.partner', ignore_fields = ignore_fields, workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract crm.lead.tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_data('crm.lead.tag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load crm.lead.tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('crm.lead.tag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract crm.stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_data('crm.stage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load crm.stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('crm.stage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract and transform crm.team**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'crm.team'\n",
    "export_data(model_name)\n",
    "\n",
    "#Export team members\n",
    "fields_to_export = ['id', 'member_ids', 'member_ids/id']\n",
    "export_data(model_name=model_name,\n",
    "            fields=fields_to_export,\n",
    "            output_file=CRM_TEAM_MEMBERS_FILE_NAME\n",
    "            )\n",
    "crm_team_file_path = f'{GENERATED_CSV_FILES_PATH}{CRM_TEAM_MEMBERS_FILE_NAME}'\n",
    "crm_team_dataframe = pandas.read_csv(crm_team_file_path)\n",
    "crm_team_dataframe.fillna(method='ffill', inplace=True)\n",
    "crm_team_dataframe.to_csv(crm_team_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load crm.team**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'crm.team'\n",
    "import_data(model_name)\n",
    "import_data(model_name=model_name, ignore_fields=['member_ids'], file_csv=CRM_TEAM_MEMBERS_FILE_NAME, workers=1, context={'update_many2many': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract and transform crm.lead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'crm.lead'\n",
    "export_data(model_name)\n",
    "\n",
    "crm_lead_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "crm_lead_dataframe = pandas.read_csv(crm_lead_file_path)\n",
    "crm_lead_dataframe.rename(columns={'planned_revenue': 'expected_revenue'}, inplace=True)\n",
    "crm_lead_dataframe['priority'] = crm_lead_dataframe['priority'].str.replace('Low', 'Medium')\n",
    "crm_lead_dataframe['priority'] = crm_lead_dataframe['priority'].str.replace('Normal', 'Low')\n",
    "crm_lead_dataframe['tag_ids/id'] = crm_lead_dataframe['tag_ids/id'].str.replace('False', '')\n",
    "crm_lead_dataframe.sort_values('id', inplace=True)\n",
    "\n",
    "crm_lead_dataframe.to_csv(crm_lead_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load crm.lead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('crm.lead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract project.tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_data('project.tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load project.tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('project.tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract and transform project.project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'project.project'\n",
    "export_data(model_name)\n",
    "\n",
    "projects_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "projects_dataframe = pandas.read_csv(projects_file_path)\n",
    "projects_dataframe['privacy_visibility'] = projects_dataframe['privacy_visibility'].str.replace('Visible by all employees', 'All employees')\n",
    "projects_dataframe['privacy_visibility'] = projects_dataframe['privacy_visibility'].str.replace('On invitation only', 'Invited employees')\n",
    "projects_dataframe.to_csv(projects_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load project.project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'project.project'\n",
    "import_data(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract project.task.type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'project.task.type'\n",
    "export_data(model_name)\n",
    "\n",
    "project_task_type_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "project_task_types_dataframe = pandas.read_csv(project_task_type_file_path)\n",
    "project_task_types_dataframe['project_ids/id'] = project_task_types_dataframe['project_ids/id'].str.replace('False', '')\n",
    "project_task_types_dataframe.to_csv(project_task_type_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load project.task.type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('project.task.type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract and transform project.task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'project.task'\n",
    "export_data(model_name)\n",
    "\n",
    "project_tasks_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "project_tasks_dataframe = pandas.read_csv(project_tasks_file_path)\n",
    "project_tasks_dataframe['user_id/id'] = project_tasks_dataframe['user_id/id'].str.replace('False', '')\n",
    "project_tasks_dataframe['tag_ids/id'] = project_tasks_dataframe['tag_ids/id'].str.replace('False', '')\n",
    "project_tasks_dataframe['priority'] = project_tasks_dataframe['priority'].str.replace('Normal', 'Important')\n",
    "project_tasks_dataframe['priority'] = project_tasks_dataframe['priority'].str.replace('Low', 'Normal')\n",
    "project_tasks_dataframe.rename(columns={'user_id/id': 'user_ids/id'}, inplace=True)\n",
    "project_tasks_dataframe.rename(columns={'email_from': 'email_cc'}, inplace=True)\n",
    "project_tasks_dataframe.to_csv(project_tasks_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load project.task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'project.task'\n",
    "import_data(model_name)\n",
    "import_ignored_fields(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract product.category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.category'\n",
    "export_data(model_name)\n",
    "\n",
    "categories_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "categories_dataframe = pandas.read_csv(categories_file_path)\n",
    "\n",
    "categories_dataframe['id'] = categories_dataframe['id'].str.replace('hr_expense.cat_expense', 'product.cat_expense')\n",
    "\n",
    "categories_dataframe.to_csv(categories_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load product.category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.category'\n",
    "import_data(model_name)\n",
    "import_ignored_fields(model_name, group_by='parent_id/id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract and Transform product.uom.categ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.uom.categ'\n",
    "export_data(model_name)\n",
    "\n",
    "categories_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "categories_dataframe = pandas.read_csv(categories_file_path)\n",
    "\n",
    "categories_dataframe['id'] = categories_dataframe['id'].str.replace('product.product_uom_categ_unit', 'uom.product_uom_categ_unit')\n",
    "categories_dataframe['id'] = categories_dataframe['id'].str.replace('product.product_uom_categ_kgm', 'uom.product_uom_categ_kgm')\n",
    "categories_dataframe['id'] = categories_dataframe['id'].str.replace('product.uom_categ_wtime', 'uom.uom_categ_wtime')\n",
    "categories_dataframe['id'] = categories_dataframe['id'].str.replace('product.uom_categ_length', 'uom.uom_categ_length')\n",
    "categories_dataframe['id'] = categories_dataframe['id'].str.replace('product.product_uom_categ_vol', 'uom.product_uom_categ_vol')\n",
    "\n",
    "categories_dataframe.to_csv(categories_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "**Load uom.category** (New Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('uom.category', file_csv='product.uom.categ.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract product.uom**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.uom'\n",
    "export_data(model_name)\n",
    "\n",
    "uoms_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "uoms_dataframe = pandas.read_csv(uoms_file_path)\n",
    "\n",
    "uoms_dataframe['category_id/id'] = uoms_dataframe['category_id/id'].str.replace('product.product_', 'uom.product_')\n",
    "uoms_dataframe['category_id/id'] = uoms_dataframe['category_id/id'].str.replace('product.uom_categ_wtime', 'uom.uom_categ_wtime')\n",
    "uoms_dataframe['category_id/id'] = uoms_dataframe['category_id/id'].str.replace('product.uom_categ_length', 'uom.uom_categ_length')\n",
    "uoms_dataframe['id'] = uoms_dataframe['id'].str.replace('product.product_', 'uom.product_')\n",
    "\n",
    "uoms_dataframe.to_csv(uoms_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load uom.uom** (New Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('uom.uom', file_csv='product.uom.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract product.template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.template'\n",
    "export_data(model_name)\n",
    "\n",
    "products_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "products_dataframe = pandas.read_csv(products_file_path)\n",
    "\n",
    "products_dataframe.rename(columns={'type': 'detailed_type'}, inplace=True)\n",
    "products_dataframe['detailed_type'] = products_dataframe['detailed_type'].str.replace('Stockable Product', 'Storable Product')\n",
    "products_dataframe['uom_id/id'] = products_dataframe['uom_id/id'].str.replace('product.product_', 'uom.product_')\n",
    "products_dataframe['uom_po_id/id'] = products_dataframe['uom_po_id/id'].str.replace('product.product_', 'uom.product_')\n",
    "products_dataframe['default_code'] = products_dataframe['default_code'].astype(str)\n",
    "\n",
    "products_dataframe.to_csv(products_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load product.template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.template'\n",
    "import_data(model_name, workers=1, context={'create_product_product': True, 'tracking_disable': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract product.product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.product'\n",
    "export_data(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load product.product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.product'\n",
    "import_data(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract product.supplierinfo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'product.supplierinfo'\n",
    "export_data(model_name)\n",
    "\n",
    "supplierinfo_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "supplierinfo_dataframe = pandas.read_csv(supplierinfo_file_path)\n",
    "\n",
    "supplierinfo_dataframe['product_uom/id'] = supplierinfo_dataframe['product_uom/id'].str.replace('product.product_', 'uom.product_')\n",
    "\n",
    "supplierinfo_dataframe.to_csv(supplierinfo_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load product.supplierinfo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('product.supplierinfo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract purchase.order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'purchase.order'\n",
    "\n",
    "export_data(model_name)\n",
    "\n",
    "purchases_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "purchases_dataframe = pandas.read_csv(purchases_file_path)\n",
    "\n",
    "purchases_dataframe.rename(columns={'buyer/id': 'user_id/id'}, inplace=True)\n",
    "purchases_dataframe['state'] = purchases_dataframe['state'].str.replace('Draft PO', 'RFQ')\n",
    "purchases_dataframe['invoice_status'] = purchases_dataframe['invoice_status'].str.replace('No Bill to Receive', 'Fully Billed')\n",
    "purchases_dataframe.sort_values('id', inplace=True)\n",
    "\n",
    "purchases_dataframe.to_csv(purchases_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load purchase.order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('purchase.order', workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract purchase.order.line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'purchase.order.line'\n",
    "\n",
    "export_data(model_name)\n",
    "\n",
    "purchase_lines_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "purchase_lines_dataframe = pandas.read_csv(purchase_lines_file_path)\n",
    "\n",
    "purchase_lines_dataframe['product_uom/id'] = purchase_lines_dataframe['product_uom/id'].str.replace('product.product_', 'uom.product_')\n",
    "purchase_lines_dataframe['taxes_id/id'] = purchase_lines_dataframe['taxes_id/id'].astype(str).str.replace('False', '')\n",
    "\n",
    "purchase_lines_dataframe.sort_values('id', inplace=True)\n",
    "\n",
    "purchase_lines_dataframe.to_csv(purchase_lines_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load purchase.order.line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('purchase.order.line', workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract sale.order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'sale.order'\n",
    "\n",
    "export_data(model_name)\n",
    "sales_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "sales_dataframe = pandas.read_csv(sales_file_path)\n",
    "\n",
    "sales_dataframe.date_order[sales_dataframe.state == 'Sale Order'] = sales_dataframe.confirmation_date\n",
    "sales_dataframe.date_order[sales_dataframe.state == 'Done'] = sales_dataframe.confirmation_date\n",
    "sales_dataframe.date_order[sales_dataframe.date_order.isnull()] = sales_dataframe.create_date\n",
    "\n",
    "sales_dataframe['state'] = sales_dataframe['state'].str.replace('Sale Order', 'Sales Order')\n",
    "sales_dataframe['state'] = sales_dataframe['state'].str.replace('On Hold', 'Quotation Sent')\n",
    "sales_dataframe['state'] = sales_dataframe['state'].str.replace('CC Hold', 'Quotation Sent')\n",
    "sales_dataframe['state'] = sales_dataframe['state'].str.replace('Approved', 'Quotation Sent')\n",
    "sales_dataframe.drop(columns={'confirmation_date'}, inplace=True)\n",
    "\n",
    "sales_dataframe['picking_policy'] = sales_dataframe['picking_policy'].str.replace('Deliver each product when available', 'As soon as possible')\n",
    "sales_dataframe['picking_policy'] = sales_dataframe['picking_policy'].str.replace('Deliver all products at once', 'When all products are ready')\n",
    "sales_dataframe['invoice_status'] = sales_dataframe['invoice_status'].str.replace('No Bill to Receive', 'Fully Billed')\n",
    "sales_dataframe['invoice_status'] = sales_dataframe['invoice_status'].str.replace('False', '')\n",
    "sales_dataframe.sort_values('id', inplace=True)\n",
    "\n",
    "sales_dataframe.to_csv(sales_file_path, index=False)\n",
    "\n",
    "#Shipping Addresses\n",
    "shipping_addresses_file_name = 'shipping_addresses.csv'\n",
    "shipping_addresses_file_path = f'{GENERATED_CSV_FILES_PATH}{shipping_addresses_file_name}'\n",
    "\n",
    "export_data(\n",
    "    model_name,\n",
    "    output_file=shipping_addresses_file_name,\n",
    "    fields=['id', 'partner_id/id', 'shipping_contact_name', 'shipping_street', 'shipping_city', 'shipping_zip', 'shipping_state_id/id', 'shipping_country_id/id']\n",
    ")\n",
    "addresses_dataframe = pandas.read_csv(shipping_addresses_file_path)\n",
    "\n",
    "addresses_dataframe.rename(\n",
    "    columns={\n",
    "        'id': 'so_id',\n",
    "        'partner_id/id': 'parent_id/id',\n",
    "        'shipping_contact_name': 'name',\n",
    "        'shipping_street': 'street',\n",
    "        'shipping_city': 'city',\n",
    "        'shipping_state_id/id': 'state_id/id',\n",
    "        'shipping_zip': 'zip',\n",
    "        'shipping_country_id/id': 'country_id/id'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# States remapping\n",
    "states_remapping_file_path = f'{INPUT_CSV_FILES_PATH}{STATES_REMAPPING_FILE_NAME}'\n",
    "states_remapping_dataframe = pandas.read_csv(states_remapping_file_path)\n",
    "\n",
    "for row in states_remapping_dataframe.itertuples():\n",
    "    indexes = addresses_dataframe[addresses_dataframe['state_id/id'] == row.flash_hlg_state_id].index.tolist()\n",
    "    for i in indexes:\n",
    "        state_id = row.id if type(row.id) == str else ''\n",
    "        addresses_dataframe.loc[i:i, 'state_id/id': 'country_id/id'] = state_id, row.country_id\n",
    "\n",
    "addresses_dataframe['id'] = \"\"\n",
    "\n",
    "import sys\n",
    "# To contemplate duplicate addresses, the external id of the new res.partner is set using a hash code generated from the customer id and all address values\n",
    "addresses_dataframe['id'] = addresses_dataframe[addresses_dataframe.columns.difference(['so_id'])]. \\\n",
    "    apply(\n",
    "        lambda x: f'migration_data.res_partner_{hash(tuple(x)) + sys.maxsize + 1}', # we add maxsize + 1 to make the hash values all possitive\n",
    "        axis = 1,\n",
    "        result_type='broadcast'\n",
    "    )['id']\n",
    "# Copy the address id and the so id before removing duplicate addresses\n",
    "sales_addresses_dataframe = addresses_dataframe[['id', 'so_id']].copy()\n",
    "\n",
    "addresses_dataframe.drop_duplicates('id', inplace=True)\n",
    "addresses_dataframe['type'] = 'delivery'\n",
    "addresses_dataframe.to_csv(shipping_addresses_file_path, index=False)\n",
    "\n",
    "sales_addresses_dataframe.rename(columns={'id': 'partner_shipping_id/id'}, inplace=True)\n",
    "sales_addresses_dataframe.rename(columns={'so_id': 'id'}, inplace=True)\n",
    "sales_addresses_dataframe.to_csv(f'{GENERATED_CSV_FILES_PATH}sales_addresses.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load sale.order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('sale.order', ignore_fields=['state'], workers=1)\n",
    "#Load SO's addresses\n",
    "import_data(\n",
    "    model_name='res.partner',\n",
    "    file_csv='shipping_addresses.csv',\n",
    "    ignore_fields=['so_id'],\n",
    "    group_by='parent_id/id',\n",
    "    workers=1\n",
    ")\n",
    "import_data('sale.order', file_csv='sales_addresses.csv', workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract sale.order.line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'sale.order.line'\n",
    "\n",
    "export_data(model_name)\n",
    "\n",
    "sale_lines_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "sale_lines_dataframe = pandas.read_csv(sale_lines_file_path)\n",
    "\n",
    "sale_lines_dataframe['product_uom/id'] = sale_lines_dataframe['product_uom/id'].str.replace('product.product_', 'uom.product_')\n",
    "sale_lines_dataframe['tax_id/id'] = sale_lines_dataframe['tax_id/id'].astype(str).str.replace('False', '')\n",
    "sale_lines_dataframe['tax_id/id'] = sale_lines_dataframe['tax_id/id'].astype(str).str.replace('__export__.account_tax_5_b7c630ad', '')\n",
    "\n",
    "sale_lines_dataframe.sort_values('id', inplace=True)\n",
    "\n",
    "sale_lines_dataframe.to_csv(sale_lines_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load sale.order.line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('sale.order.line', workers=1)\n",
    "import_data('sale.order')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract ir.sequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_data('ir.sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load ir.sequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('ir.sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract ir.sequence.date_range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'ir.sequence.date_range'\n",
    "\n",
    "export_data(model_name)\n",
    "\n",
    "sequence_date_ranges_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "sequence_date_ranges_dataframe = pandas.read_csv(sequence_date_ranges_file_path)\n",
    "\n",
    "sequences_file_path = f'{GENERATED_CSV_FILES_PATH}ir.sequence.csv'\n",
    "sequences_dataframe = pandas.read_csv(sequences_file_path)\n",
    "sequences_dataframe.rename(columns={'id': 'sequence_id/id'}, inplace=True)\n",
    "\n",
    "sequence_date_ranges_dataframe = sequence_date_ranges_dataframe.merge(sequences_dataframe['sequence_id/id'], on=['sequence_id/id'], how='inner')\n",
    "\n",
    "sequence_date_ranges_dataframe.to_csv(sequence_date_ranges_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load ir.sequence.date_range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('ir.sequence.date_range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "**Extract and transform ir.model.data**\n",
    "Requires all the previous steps to work properly given that it depends on the new external ids created on the new Odoo instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_to_export_external_ids = ['crm.lead', 'crm.team', 'res.partner', 'project.project', 'project.task', 'product.template', 'purchase.order', 'sale.order', 'sale.order.line']\n",
    "#Export external_ids\n",
    "fields_to_export = ['complete_name', 'res_id', 'model']\n",
    "export_data(model_name='ir.model.data',\n",
    "            fields=fields_to_export,\n",
    "            output_file='ir.model.data.old.csv',\n",
    "            domain=[['model', 'in', models_to_export_external_ids]]\n",
    "            )\n",
    "#Export v15 external_ids\n",
    "export_data(config='import_connection.conf',\n",
    "            model_name='ir.model.data',\n",
    "            fields=fields_to_export,\n",
    "            output_file='ir.model.data.new.csv',\n",
    "            domain=[['model', 'in', models_to_export_external_ids]]\n",
    "            )\n",
    "external_ids_old_file_path = f'{GENERATED_CSV_FILES_PATH}ir.model.data.old.csv'\n",
    "external_ids_new_file_path = f'{GENERATED_CSV_FILES_PATH}ir.model.data.new.csv'\n",
    "merged_data_frame_file_path = f'{GENERATED_CSV_FILES_PATH}ir.model.data.merged.csv'\n",
    "ir_model_data_old_dataframe = pandas.read_csv(external_ids_old_file_path)\n",
    "ir_model_data_new_dataframe = pandas.read_csv(external_ids_new_file_path)\n",
    "ir_model_data_old_dataframe['complete_name'] = ir_model_data_old_dataframe['complete_name'].str.replace('base.partner_root', 'base.partner_admin')\n",
    "ir_model_data_old_dataframe['complete_name'] = ir_model_data_old_dataframe['complete_name'].str.replace('base.default_user_res_partner', 'base.template_portal_user_id_res_partner')\n",
    "ir_model_data_old_dataframe.to_csv(external_ids_old_file_path, index=False)\n",
    "# Merge old and new external ids to extract the new database id\n",
    "ir_model_data_old_dataframe.rename(columns={'res_id': 'old_res_id'}, inplace=True)\n",
    "ir_model_data_merged_dataframe = ir_model_data_new_dataframe.merge(ir_model_data_old_dataframe, on=['complete_name', 'model'], how='inner')\n",
    "ir_model_data_merged_dataframe.rename(columns={'res_id': 'new_res_id'}, inplace=True)\n",
    "ir_model_data_merged_dataframe['old_res_id'] = ir_model_data_merged_dataframe['old_res_id'].astype('int')\n",
    "ir_model_data_merged_dataframe.to_csv(merged_data_frame_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract and transform mail.message**\n",
    "Requires all the previous steps to work properly given that it depends on the new external ids created on the new Odoo instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'mail.message'\n",
    "export_data(model_name)\n",
    "\n",
    "mail_message_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "mail_message_dataframe = pandas.read_csv(mail_message_file_path)\n",
    "\n",
    "mail_message_dataframe['subtype_id/id'] = mail_message_dataframe['subtype_id/id'].str.replace('False', '')\n",
    "mail_message_dataframe['partner_ids/id'] = mail_message_dataframe['partner_ids/id'].astype('str')\n",
    "mail_message_dataframe['partner_ids/id'] = mail_message_dataframe['partner_ids/id'].str.replace('False', '')\n",
    "\n",
    "ir_model_data_merged_file_path = f'{GENERATED_CSV_FILES_PATH}ir.model.data.merged.csv'\n",
    "ir_model_data_merged_dataframe = pandas.read_csv(ir_model_data_merged_file_path)\n",
    "\n",
    "# Merge the mail.message dataframe with ir_model_data_merged_dataframe to get the new database ids\n",
    "mail_message_dataframe = mail_message_dataframe.merge(ir_model_data_merged_dataframe, left_on=['model', 'res_id'], right_on=['model', 'old_res_id'], how='inner')\n",
    "mail_message_dataframe.rename(columns={'res_id': 'old_res_id'}, inplace=True)\n",
    "mail_message_dataframe.rename(columns={'new_res_id': 'res_id'}, inplace=True)\n",
    "mail_message_dataframe['res_id'] = mail_message_dataframe['res_id'].astype('int')\n",
    "\n",
    "mail_message_dataframe.sort_values('date', ascending=True, inplace=True)\n",
    "mail_message_dataframe.to_csv(mail_message_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load mail.message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'mail.message'\n",
    "ignored_fields = ['old_res_id', 'old_res_id2', 'complete_name', 'parent_id/id']\n",
    "import_data(model_name=model_name, ignore_fields=ignored_fields, group_by='complete_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract and transform mail.tracking.value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'mail.tracking.value'\n",
    "mail_tracking_value_all_file_name = f'{model_name}.all.csv'\n",
    "mail_tracking_value_all_file_path = f'{GENERATED_CSV_FILES_PATH}{mail_tracking_value_all_file_name}'\n",
    "mail_tracking_value_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "export_data(model_name=model_name, output_file=mail_tracking_value_all_file_name)\n",
    "mail_tracking_value_dataframe = pandas.read_csv(mail_tracking_value_all_file_path, low_memory=False)\n",
    "\n",
    "mail_message_file_path = f'{GENERATED_CSV_FILES_PATH}mail.message.csv'\n",
    "mail_message_dataframe = pandas.read_csv(mail_message_file_path, low_memory=False)\n",
    "mail_message_dataframe.rename(columns={'id': 'mail_message_id/id'}, inplace=True)\n",
    "\n",
    "#Merge tracking values with mail.message to filter the tracking values to import and adding the model column\n",
    "mail_message_dataframe = mail_message_dataframe[['model', 'mail_message_id/id']]\n",
    "mail_tracking_value_dataframe = mail_tracking_value_dataframe.merge(mail_message_dataframe, on='mail_message_id/id', how='inner')\n",
    "\n",
    "#Export v15 ir_model_fields\n",
    "fields_model_name = 'ir.model.fields'\n",
    "models_with_mail_messages = list(mail_message_dataframe['model'].unique())\n",
    "models_to_export_external_ids = [m for m in models_with_mail_messages if m in models_migration_config.keys()]\n",
    "export_data(config='import_connection.conf',\n",
    "            model_name=fields_model_name,\n",
    "            fields=['id', 'name', 'model'],\n",
    "            output_file=f'{fields_model_name}.csv',\n",
    "            domain=[['model', 'in', models_to_export_external_ids]]\n",
    "            )\n",
    "ir_model_fields_file_path = f'{GENERATED_CSV_FILES_PATH}{fields_model_name}.csv'\n",
    "fields_dataframe = pandas.read_csv(ir_model_fields_file_path)\n",
    "fields_dataframe.rename(columns={'id': 'field/id'}, inplace=True)\n",
    "fields_dataframe.rename(columns={'name': 'field'}, inplace=True)\n",
    "\n",
    "#Merge tracking values with fields to extract the fields' external ids\n",
    "mail_tracking_value_dataframe['field'] = mail_tracking_value_dataframe['field'].str.replace('planned_revenue', 'expected_revenue')\n",
    "mail_tracking_value_dataframe['field'] = mail_tracking_value_dataframe['field'].str.replace('categ_id', 'category_id')\n",
    "mail_tracking_value_dataframe['field'] = mail_tracking_value_dataframe['field'].str.replace('salesperson_ids', 'user_id')\n",
    "mail_tracking_value_dataframe = mail_tracking_value_dataframe.merge(fields_dataframe, on=['field', 'model'], how='inner')\n",
    "# Drop columns used just to merge\n",
    "mail_tracking_value_dataframe.drop(columns={'field', 'model'}, inplace=True)\n",
    "\n",
    "mail_tracking_value_dataframe.to_csv(mail_tracking_value_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load mail.tracking.value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data('mail.tracking.value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract ir.attachment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'ir.attachment'\n",
    "export_data(model_name)\n",
    "\n",
    "ir_attachment_file_path = f'{GENERATED_CSV_FILES_PATH}{model_name}.csv'\n",
    "ir_attachment_dataframe = pandas.read_csv(ir_attachment_file_path)\n",
    "\n",
    "ir_model_data_merged_file_path = f'{GENERATED_CSV_FILES_PATH}ir.model.data.merged.csv'\n",
    "ir_model_data_merged_dataframe = pandas.read_csv(ir_model_data_merged_file_path)\n",
    "ir_model_data_merged_dataframe.rename(columns={'model': 'res_model'}, inplace=True)\n",
    "\n",
    "# Merge the ir.attachment dataframe with ir_model_data_merged_dataframe to get the new database ids\n",
    "ir_attachment_dataframe.rename(columns={'res_id': 'old_res_id'}, inplace=True)\n",
    "ir_attachment_dataframe = ir_attachment_dataframe.merge(ir_model_data_merged_dataframe, on=['res_model', 'old_res_id'], how='inner')\n",
    "ir_attachment_dataframe.drop(columns={'old_res_id', 'complete_name'}, inplace=True)\n",
    "ir_attachment_dataframe.rename(columns={'new_res_id': 'res_id'}, inplace=True)\n",
    "\n",
    "ir_attachment_dataframe.sort_values('id', ascending=True, inplace=True)\n",
    "ir_attachment_dataframe.to_csv(f'{GENERATED_CSV_FILES_PATH}{model_name}2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Extract ir.attachment** Part 2\n",
    "We separate this process into 2 parts to keep the output shorter, in part 1 all attachments references are loaded but with empty files, in this second part the actual files data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Separate into batches per model to keep files smaller and avoid memory errors\n",
    "attachments_model_name = 'ir.attachment'\n",
    "fields_to_export = ['id', 'name', 'datas']\n",
    "models_with_attachments = ['project.task', 'crm.lead', 'res.partner', 'product.template']\n",
    "max_file_size_for_batches = 50000000 # 50 MB\n",
    "batch_size = 20 # Some files are very large so it fails due to 413 (Payload Too Large), so we keep the batch size small\n",
    "\n",
    "export_data(\n",
    "    attachments_model_name,\n",
    "    fields=fields_to_export,\n",
    "    domain=[['res_model', 'in', models_with_attachments], ['file_size', '>', max_file_size_for_batches]],\n",
    "    output_file=f'{attachments_model_name}.big_files.csv',\n",
    "    batch_size=1\n",
    ")\n",
    "for model_name in models_with_attachments:\n",
    "    export_data(\n",
    "        attachments_model_name,\n",
    "        fields=fields_to_export,\n",
    "        domain=[['res_model', '=', model_name], ['file_size', '<', max_file_size_for_batches]],\n",
    "        output_file=f'{attachments_model_name}.{model_name}.csv',\n",
    "        batch_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Load ir.attachment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attachments_model_name = 'ir.attachment'\n",
    "batch_size = 15\n",
    "models_with_attachments = ['project.task', 'crm.lead', 'res.partner', 'product.template']\n",
    "\n",
    "import_data(attachments_model_name, file_csv=f'{attachments_model_name}2.csv')\n",
    "import_data(attachments_model_name, file_csv=f'{attachments_model_name}.big_files.csv', batch_size=1)\n",
    "for model_name in models_with_attachments:\n",
    "    import_data(attachments_model_name, file_csv=f'{attachments_model_name}.{model_name}.csv', batch_size=batch_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Vendors from I5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_address_into_different_columns():\n",
    "    vendors_file_path = f'{INPUT_CSV_FILES_PATH}APMASTER - Vendors.tsv'\n",
    "    vendors_dataframe = pandas.read_csv(vendors_file_path, sep='\\t')\n",
    "\n",
    "    # AMVEND\tAMNAME\tAMADR1\tAMADR2\tAMADR3\tAMTELE\tAMFRGT\tAMTERM\n",
    "    # 1\tBROAN MFG OR EDN              \t4641 PAYSPHERE CIRCLE    \tCHICAGO,IL 60674         \t                         \t8778470145\t1250\t2%10TH \n",
    "    # 172\tHUNTER FAN COMPANY            \tP O BOX 19773            \t                         \tPALATINE IL 60055-9773   \t9017441200\t1000\t1%2OTH \n",
    "    # 10003\tLEDVANCE OR IMARK             \tP O BOX 72524            \t(OSRAM)                  \tCLEVELAND OH 44192       \t8002555042\t1000\t2%90D  \n",
    "    # 10004\tADVANCE TRANSFORMER CO        \tP O BOX 100332           \tATLANTA GA  30384        \t                         \t0\t750\t2%10TH \n",
    "    # 10005\tLUTRON ELECTRONICS CO INC     \tP O BOX 644396           \t                         \tPITTSBURGH, PA 15264-4396\t8005239466\t500\t1%10TH \n",
    "\n",
    "    col_names = ['AMVEND', 'AMNAME', 'AMADR1', 'AMADR2', 'AMADR3', 'AMTELE', 'AMFRGT', 'AMTERM']\n",
    "    for col_name in col_names:\n",
    "        vendors_dataframe[col_name] = vendors_dataframe[col_name].astype(str)\n",
    "        vendors_dataframe[col_name] = vendors_dataframe[col_name].str.strip()\n",
    "\n",
    "    #Change AMADR1 column to the end for better visibility\n",
    "    vendors_dataframe = vendors_dataframe[['AMVEND', 'AMNAME', 'AMADR2', 'AMADR3', 'AMTELE', 'AMFRGT', 'AMTERM', 'AMADR1']]\n",
    "    vendors_dataframe.rename(columns={'AMADR1': 'street'}, inplace=True)\n",
    "\n",
    "    vendors_dataframe['street2'] = ''\n",
    "    vendors_dataframe['city'] = ''\n",
    "    vendors_dataframe['state_id'] = ''\n",
    "    vendors_dataframe['zip'] = ''\n",
    "\n",
    "    import re\n",
    "    def is_zip(zip_code : str) -> bool:\n",
    "        \"\"\"\n",
    "        >>> is_zip('6067')\n",
    "        True\n",
    "        >>> is_zip('60674')\n",
    "        True\n",
    "        >>> is_zip('60674-1234')\n",
    "        True\n",
    "        \"\"\"\n",
    "        return bool(re.match(r'^\\d{5}(-\\d{4})?$', zip_code)) or bool(re.match(r'^\\d{4}', zip_code))\n",
    "\n",
    "    us_states = {\n",
    "        'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY',\n",
    "        'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY',\n",
    "        'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
    "    }\n",
    "\n",
    "    def is_a_state(state : str) -> bool:\n",
    "        return state in us_states\n",
    "\n",
    "    def clean_address_field(address_field : str) -> str:\n",
    "        address_field = address_field.strip()\n",
    "        address_field = address_field.replace('     ', ' ')\n",
    "        address_field = address_field.replace('    ', ' ')\n",
    "        address_field = address_field.replace('   ', ' ')\n",
    "        address_field = address_field.replace('  ', ' ')\n",
    "        address_field = address_field.replace(',', ' ')\n",
    "        address_field = address_field.replace('  ', ' ')\n",
    "        return address_field\n",
    "\n",
    "    for row_index, row in vendors_dataframe.iterrows():\n",
    "        row_address2 = row.AMADR2 or ''\n",
    "        row_address2 = clean_address_field(row_address2)\n",
    "        row_address3 = row.AMADR3 or ''\n",
    "        row_address3 = clean_address_field(row_address3)\n",
    "\n",
    "        if row_address3:\n",
    "            row.street2 = row_address2\n",
    "            city_state_zip = row_address3\n",
    "        else:\n",
    "            city_state_zip = row_address2\n",
    "        if city_state_zip:\n",
    "            city_state_zip_splitted = city_state_zip.split(' ')[::-1]\n",
    "\n",
    "            for i, address_field in enumerate(city_state_zip_splitted):\n",
    "                if is_zip(address_field):\n",
    "                    row.zip = address_field\n",
    "                elif is_a_state(address_field):\n",
    "                    row.state_id = address_field\n",
    "                else:\n",
    "                    row.city = ' '.join(city_state_zip_splitted[i:][::-1])\n",
    "                    break\n",
    "            if not row.zip or not row.state_id or not row.city and row.street2:\n",
    "                city_state_zip_splitted = row.street2.split(' ')[::-1]\n",
    "                for i, address_field in enumerate(city_state_zip_splitted):\n",
    "                    if not row.zip and is_zip(address_field):\n",
    "                        row.zip = address_field\n",
    "                    elif not row.state_id and is_a_state(address_field):\n",
    "                        row.state_id = address_field\n",
    "                    elif not row.city:\n",
    "                        row.city = ' '.join(city_state_zip_splitted[i:][::-1])\n",
    "                        break\n",
    "    vendors_dataframe.to_csv(f'{GENERATED_CSV_FILES_PATH}i5.vendors.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "separate_address_into_different_columns()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "dca556ecc5dd513367558f6312acb415247f7e6342b10809f71b9a3671f6e71e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
